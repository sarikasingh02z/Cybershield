##CyberShield NIDS
###Experimental Network Intrusion Detection System using Machine Learning

##Overview
CyberShield is an experimental, research-oriented Network Intrusion Detection System (NIDS) that investigates how threshold tuning and error analysis affect detection reliability in machine learning–based security systems under imbalanced network traffic.
Rather than presenting a production-ready security system, this project focuses on methodological evaluation, controlled experimentation, and interpretability of model behavior, which are critical for applied machine learning research in cybersecurity.

##Research Motivation
-Machine learning–based intrusion detection systems often report strong accuracy in offline evaluation, yet default decision thresholds frequently lead to unstable performance, excessive false positives, or missed attacks in realistic traffic distributions.
-Research Question: How does confidence threshold tuning influence false-positive rates and detection stability in a dual-model intrusion detection system trained on imbalanced network traffic?
-CyberShield is intentionally designed as an experimental testbed to analyze this question through systematic evaluation.

##System Architecture
###CyberShield employs a dual-model detection pipeline to separate anomaly identification from attack classification:
-Isolation Forest (Unsupervised)
Used to identify anomalous network behavior without reliance on labeled attack data.
=Random Forest (Supervised)
Used for intrusion detection and attack category prediction on labeled traffic.

###Model predictions are combined with:
-a configurable confidence threshold (θ)
-feature-level traffic constraints (rate, byte volume, TTL, duration)

This layered decision logic enables controlled experimentation on detection sensitivity and false-positive behavior.

##Attack Categories

The system evaluates traffic across the following categories:
-Normal
-DDoS
-Port Scan
-Brute Force
-Spoofing
-Data Exfiltration
-Slow Attack
-Fast Flux

##Experimental Setup
-Total samples: 175,341 network flows
-Traffic distribution: Highly imbalanced
-Evaluation mode: Controlled experimental analysis
-Objective: Stability and error behavior, not raw accuracy
-All experiments were conducted using fixed train–test splits to ensure comparability across thresholds.

##Threshold Sensitivity Experiment
A systematic threshold sensitivity analysis was performed by evaluating multiple confidence thresholds between θ = 0.10 and θ = 0.85.

###Results Summary
Threshold     	|F1 Score
0.50 (default)	| 93.34%
0.30 (optimized)|94.73%

-Improvement: +1.39% F1 score
-Selected operational threshold: θ = 0.30

This result demonstrates that commonly used default thresholds are suboptimal for imbalanced intrusion detection tasks.

##Optimized Performance Metrics (θ = 0.30)
-Accuracy: 92.76%
-Precision: 94.01%
-Recall: 95.45%
-F1 Score: 94.73%
-False Positive Rate: ~4.1%

These metrics reflect a balanced tradeoff between detection sensitivity and false alarm control.
